---
content_type: resource
description: ''
file: null
resourcetype: Video
title: 'Lecture 7: Eckart-Young: The Closest Rank k Matrix to A '
uid: 7d689bdb-8da9-848c-569f-85e93ce9b9d4
video_files:
  archive_url: https://archive.org/download/MIT18.065S18/MIT18_065S18_Lecture07_300k.mp4
  video_captions_file: /courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/ae3fcb7f64fd52e7abb899b9f665d5f4_Y4f7K9XF04k.vtt
  video_thumbnail_file: https://img.youtube.com/vi/Y4f7K9XF04k/default.jpg
  video_transcript_file: /courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/70996912a170cf9c6ebb018f03c1fc85_Y4f7K9XF04k.pdf
video_metadata:
  youtube_id: Y4f7K9XF04k
---

Description
-----------

In this lecture, Professor Strang reviews Principal Component Analysis (PCA), which is a major tool in understanding a matrix of data. In particular, he focuses on the Eckart-Young low rank approximation theorem.

Summary
-------

\\(A\_k = \\sigma\_1 u\_1 v^{\\mathtt{T}}\_1 + \\cdots + \\sigma\_k u\_k v^{\\mathtt{T}}\_k\\) (larger \\(\\sigma\\)'s from \\(A = U\\Sigma V^{\\mathtt{T}}\\))  
The norm of \\(A - A\_k\\) is below the norm of all other \\(A - B\_k\\).  
Frobenius norm squared = sum of squares of all entries  
The idea of Principal Component Analysis (PCA)

Related section in textbook: I.9

**Instructor:** Prof. Gilbert Strang

p.p1 {margin: 0.0px 0.0px 0.0px 0.0px; font: 8.0px Verdana} span.s1 {font-kerning: none} span.s2 {font: 11.0px Verdana; font-kerning: none}