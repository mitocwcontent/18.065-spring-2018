---
content_type: resource
description: ''
file: null
resourcetype: Video
title: 'Lecture 27: Backpropagation: Find Partial Derivatives'
uid: a11969b7-2781-9223-9726-9718da7c7cb1
video_files:
  archive_url: https://archive.org/download/MIT18.065S18/MIT18_065S18_Lecture27_300k.mp4
  video_captions_file: /courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/7656fc94359b5298b8496f66bd7ad833_lZrIPRnoGQQ.vtt
  video_thumbnail_file: https://img.youtube.com/vi/lZrIPRnoGQQ/default.jpg
  video_transcript_file: /courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/f746508a6b1dd40c702cc4424382c7ba_lZrIPRnoGQQ.pdf
video_metadata:
  youtube_id: lZrIPRnoGQQ
---

Description
-----------

In this lecture, Professor Strang presents Professor Sraâ€™s theorem which proves the convergence of stochastic gradient descent (SGD). He then reviews backpropagation, a method to compute derivatives quickly, using the chain rule.

Summary
-------

Computational graph: Each step in computing \\(F(x)\\) from the weights  
Derivative of each step + chain rule gives gradient of \\(F\\).  
Reverse mode: Backwards from output to input  
The key step to optimizing weights is backprop + stoch grad descent.

Related section in textbook: VII.3

**Instructor:** Prof. Gilbert Strang