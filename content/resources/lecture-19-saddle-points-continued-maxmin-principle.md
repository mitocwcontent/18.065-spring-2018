---
content_type: resource
description: ''
file: null
resourcetype: Video
title: 'Lecture 19: Saddle Points Continued, Maxmin Principle'
uid: 1a9b96a6-8484-56dd-2b38-0557ed66a416
video_files:
  archive_url: https://archive.org/download/MIT18.065S18/MIT18_065S18_Lecture19_300k.mp4
  video_captions_file: /courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/0115219bc6a75aca92df48dfa5398d22_2K7CvGnebO0.vtt
  video_thumbnail_file: https://img.youtube.com/vi/2K7CvGnebO0/default.jpg
  video_transcript_file: /courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/756ed177370f2e49ffabf9c9e65a16ba_2K7CvGnebO0.pdf
video_metadata:
  youtube_id: 2K7CvGnebO0
---

Description
-----------

Professor Strang continues his discussion of saddle points, which are critical for deep learning applications. Later in the lecture, he reviews the Maxmin Principle, a decision rule used in probability and statistics to optimize outcomes.

Summary
-------

\\(x'Sx/x'x\\) has a saddle at eigenvalues between lowest / highest.  
(Max over all \\(k\\)-dim spaces) of (Min of \\(x'Sx/x'x\\)) = evalue  
Sample mean and expected mean  
Sample variance and \\(k\\)th eigenvalue variance

Related sections in textbook: III.2 and V.1

**Instructor:** Prof. Gilbert Strang