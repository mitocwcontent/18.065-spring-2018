---
content_type: resource
description: ''
file: null
resourcetype: Video
title: 'Lecture 9: Four Ways to Solve Least Squares Problems'
uid: a575de11-17f0-e13b-755d-03aacd93f61b
video_files:
  archive_url: https://archive.org/download/MIT18.065S18/MIT18_065S18_Lecture09_300k.mp4
  video_captions_file: /courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/c2eda44f0af850fabfe3caac68f26722_ZUU57Q3CFOU.vtt
  video_thumbnail_file: https://img.youtube.com/vi/ZUU57Q3CFOU/default.jpg
  video_transcript_file: /courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/a54e21fb9376e6aec61e02d0127f41ab_ZUU57Q3CFOU.pdf
video_metadata:
  youtube_id: ZUU57Q3CFOU
---

Description
-----------

In this lecture, Professor Strang details the four ways to solve least-squares problems. Solving least-squares problems comes in to play in the many applications that rely on data fitting.

Summary
-------

1.  Solve \\(A^{\\mathtt{T}} Ax = A^{\\mathtt{T}}b\\) to minimize \\(\\Vert Ax - b \\Vert^2\\)
2.  Gram-Schmidt \\(A = QR\\) leads to \\(x = R^{-1} Q^{\\mathtt{T}}b\\).
3.  The pseudoinverse directly multiplies \\(b\\) to give \\(x\\).
4.  The best \\(x\\) is the limit of \\((A^{\\mathtt{T}}A + \\delta I)^{-1} A^{\\mathtt{T}}b\\) as \\(\\delta \\rightarrow 0\\).

Related section in textbook: II.2

**Instructor:** Prof. Gilbert Strang