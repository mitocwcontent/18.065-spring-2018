---
content_type: resource
description: ''
file: null
resourcetype: Video
title: 'Lecture 21: Minimizing a Function Step by Step'
uid: 1f32e87c-b6ab-cb28-fefb-267c9a2d5796
video_files:
  archive_url: https://archive.org/download/MIT18.065S18/MIT18_065S18_Lecture21_300k.mp4
  video_captions_file: /courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/1691edd6d6b7545c9792cbc6525adceb_nvXRJIBOREc.vtt
  video_thumbnail_file: https://img.youtube.com/vi/nvXRJIBOREc/default.jpg
  video_transcript_file: /courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/4371db9866484c5b260f002c7f7de303_nvXRJIBOREc.pdf
video_metadata:
  youtube_id: nvXRJIBOREc
---

**Description**
---------------

In this lecture, Professor Strang discusses optimization, the fundamental algorithm that goes into deep learning. Later in the lecture he reviews the structure of convolutional neural networks (CNN) used in analyzing visual imagery.

**Summary**
-----------

Three terms of a Taylor series of \\(F\\)(\\(x\\)) : many variables \\(x\\)  
Downhill direction decided by first partial derivatives of \\(F\\) at \\(x\\)  
Newton's method uses higher derivatives (Hessian at higher cost).

Related sections in textbook: VI.1, VI.4

**Instructor:** Prof. Gilbert Strang